# ğŸ—ï¸ Data Warehouse & Analytics Project (PostgreSQL | Shell | GitHub)

Welcome to my end-to-end **Data Warehousing & Analytics** portfolio project! ğŸš€  
This project demonstrates how to build and automate a modern data pipeline using industry-standard practices â€” right from ingestion (CSV) to analytical modeling (Gold layer). It showcases **ETL, Data Modeling, Shell Scripting, and Analytics** â€” all in one!

---

## ğŸ” Medallion Architecture Overview

This project follows the **Medallion Architecture** structure:

| Layer     | Description                                                                 |
|-----------|-----------------------------------------------------------------------------|
| **Bronze** | Raw ingestion of CSV files into PostgreSQL                                  |
| **Silver** | Data cleansing, business logic, and standardization                         |
| **Gold**   | Analytical star schema (dimensional modeling) for reporting and insights    |

---

## ğŸ“– Project Goals

ğŸ¯ Design and implement a PostgreSQL data warehouse from scratch  
ğŸ” Build **ETL pipelines** using SQL & stored procedures  
ğŸ§¹ Apply robust **data cleaning & transformation** logic  
ğŸ“Š Develop a **Star Schema** for business-ready reporting  
âœ… Implement **quality checks** for data accuracy  
ğŸ–¥ï¸ Automate with **shell scripting** for reproducibility  
ğŸ“ˆ Connect to BI tools like Tableau for visualization  

---

## ğŸ§© Technologies Used

- PostgreSQL + SQL
- pgAdmin + psql
- Shell Scripting
- Git & GitHub
- Tableau (for Gold layer visualization)
- Markdown + Metadata Cataloging

---

## ğŸ“‚ Project Structure


---

## ğŸ”§ Key Scripts

| Script/File | Description |
|-------------|-------------|
| `init_db_and_schemas.sql` | Initializes PostgreSQL schemas |
| `create_bronze_tables.sql` | Creates bronze tables for raw ingestion |
| `load_bronze_data.sh` | Automates CSV loading via terminal |
| `etl_to_silver.sql` | Cleans and transforms data to silver layer |
| `create_gold_views.sql` | Builds star schema views |
| `quality_silver.sql` | Data quality checks on silver tables |
| `quality_gold.sql` | Validations between fact/dimension tables |

---

## ğŸ—‚ï¸ Metadata Catalogs

Detailed documentation for all tables, columns, and transformations:

- ğŸ“Œ `bronze_data_catalog.md` â†’ Raw schema
- ğŸ“Œ `silver_data_catalog.md` â†’ Cleaned and transformed schema
- ğŸ“Œ `gold_data_catalog.md` â†’ Star schema (facts + dimensions)

---

## ğŸ§ª Data Quality Checks

- Null or duplicate key validation
- Trim + formatting issues
- Date range validation
- Referential integrity between fact & dimension tables
- Sales = Quantity Ã— Price validation

Scripts:
- âœ… `quality_checks_silver.sql`
- âœ… `quality_checks_gold.sql`

---

## ğŸ“ˆ Business Use Cases

ğŸ“Š **Sales Insights**  
ğŸ“Š **Customer Segmentation**  
ğŸ“Š **Product Performance**  
ğŸ“Š **Time Series Analysis**  
ğŸ“Š **Geographic Distribution**  

---

## ğŸš€ How to Run This Project

```bash
# Step 1: Create DB + Schemas
psql -U postgres -f automation/init_db_and_schemas.sql

# Step 2: Create bronze tables
psql -U postgres -d datawarehouse -f scripts/bronze/create_bronze_tables.sql

# Step 3: Load data
bash automation/load_bronze_data.sh

# Step 4: Transform to silver
psql -U postgres -d datawarehouse -f scripts/silver/etl_to_silver.sql

# Step 5: Create gold views
psql -U postgres -d datawarehouse -f scripts/gold/create_gold_views.sql

# Step 6: Run quality checks
psql -U postgres -d datawarehouse -f quality_checks/quality_silver.sql
psql -U postgres -d datawarehouse -f quality_checks/quality_gold.sql

Let me know if you'd like this broken down into separate `.md` files (e.g., `bronze_readme.md`, `gold_readme.md`) or if you want me to upload a `.md` file directly.
